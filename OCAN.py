# -*- coding: utf-8 -*-
"""
Created on Sat May 29 15:20:18 2021

@author: shais
"""

import tensorflow.compat.v1 as tf 
tf.disable_v2_behavior() 
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler

from bg_utils import one_hot,wmatrix_init,pull_away_loss,sample_shuffle_uspv,sample_Z,draw_trend


mb_size = 100 # mini_batchsize
dim_input = 200 # input dimension (the hidden representation from lstm)


D_dim = [dim_input, 100, 50, 2] # discriminator number of neurons in each layer
G_dim = [50, 100, dim_input] # generator number of neurons in each layer
Z_dim = G_dim[0]  # Dimension of input noise

# definition labeled-data、unlabeled-data、noise-data and target-data placeholder
X_oc = tf.placeholder(tf.float32, shape=[None, dim_input]) # real benign data
Z = tf.placeholder(tf.float32, shape=[None, Z_dim])  # noise
X_tar = tf.placeholder(tf.float32, shape=[None, dim_input]) # target data (complementary data)

# define the Weights and biases of the discriminator
D_W1 = tf.Variable(wmatrix_init([D_dim[0], D_dim[1]])) # shape=(200,100)
D_b1 = tf.Variable(tf.zeros(shape=[D_dim[1]])) # shape=(100,)

D_W2 = tf.Variable(wmatrix_init([D_dim[1], D_dim[2]]))  # shape=(100,50)
D_b2 = tf.Variable(tf.zeros(shape=[D_dim[2]])) # shape=(50,)

D_W3 = tf.Variable(wmatrix_init([D_dim[2], D_dim[3]]))  # shape=(50,2)
D_b3 = tf.Variable(tf.zeros(shape=[D_dim[3]])) # shape=(2,)

theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]  

# Define the weights and biases of the generator
G_W1 = tf.Variable(wmatrix_init([G_dim[0], G_dim[1]]))  # shape=(50,100)
G_b1 = tf.Variable(tf.zeros(shape=[G_dim[1]])) # shape=(100,)

G_W2 = tf.Variable(wmatrix_init([G_dim[1], G_dim[2]]))  # shape=(100,200)
G_b2 = tf.Variable(tf.zeros(shape=[G_dim[2]])) # shape=(200,)

theta_G = [G_W1, G_W2, G_b1, G_b2]

# Define the weights and biases of pre-train net for density estimation
T_W1 = tf.Variable(wmatrix_init([D_dim[0], D_dim[1]]))  # shape=(200,100)
T_b1 = tf.Variable(tf.zeros(shape=[D_dim[1]]))  # shape=(100,)

T_W2 = tf.Variable(wmatrix_init([D_dim[1], D_dim[2]]))  # shape=(100,50)
T_b2 = tf.Variable(tf.zeros(shape=[D_dim[2]])) # shape=(50,)

T_W3 = tf.Variable(wmatrix_init([D_dim[2], D_dim[3]]))  # shape=(50,2)
T_b3 = tf.Variable(tf.zeros(shape=[D_dim[3]])) # shape=(2,)

theta_T = [T_W1, T_W2, T_W3, T_b1, T_b2, T_b3]

def generator(z):
    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)
    G_logit = tf.nn.tanh(tf.matmul(G_h1, G_W2) + G_b2)
    return G_logit

def discriminator(x):
    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) # 100 dim
    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2) #50 dim
    D_logit = tf.matmul(D_h2, D_W3) + D_b3
    D_prob = tf.nn.softmax(D_logit)
    return D_prob, D_logit, D_h2

# pre-train net for density estimation  
# Estimate the distribution of P_data() in order to calculate P_data(G(z))> threshold
def discriminator_tar(x):
    T_h1 = tf.nn.relu(tf.matmul(x, T_W1) + T_b1) 
    T_h2 = tf.nn.relu(tf.matmul(T_h1, T_W2) + T_b2)
    T_logit = tf.matmul(T_h2, T_W3) + T_b3
    T_prob = tf.nn.softmax(T_logit)
    return T_prob, T_logit, T_h2



D_prob_real, D_logit_real, D_h2_real = discriminator(X_oc) # Real data passes through the calculated intermediate value of the discriminator

G_sample = generator(Z)
D_prob_gen, D_logit_gen, D_h2_gen = discriminator(G_sample) # Fake data is generated by noise, and then passed through the discriminator

D_prob_tar, D_logit_tar, D_h2_tar = discriminator_tar(X_tar) # target data passes the discriminator_tar
D_prob_tar_gen, D_logit_tar_gen, D_h2_tar_gen = discriminator_tar(G_sample) # Fake data is generated by noise and passed through the discriminator _tar

# loss of discriminator
y_real = tf.placeholder(tf.int32, shape=[None, D_dim[3]])  # Labels corresponding to real data
y_gen = tf.placeholder(tf.int32, shape=[None, D_dim[3]])  # Labels corresponding to fake data

D_loss_real = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=D_logit_real,labels=y_real))
D_loss_gen = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=D_logit_gen, labels=y_gen))

ent_real_loss = -tf.reduce_mean(tf.reduce_sum(tf.multiply(D_prob_real, tf.log(D_prob_real)), 1))# conditional entropy for detecting benign users with high confidence

ent_gen_loss = -tf.reduce_mean(tf.reduce_sum(tf.multiply(D_prob_gen, tf.log(D_prob_gen)), 1))

D_loss = D_loss_real + D_loss_gen + 1.85*ent_real_loss

# loss of generator
pt_loss = pull_away_loss(D_h2_tar_gen) # PT_term, minimize this term approximately minimize -H(P_G)

y_tar = tf.placeholder(tf.int32, shape=[None, D_dim[3]])
T_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=D_logit_tar, labels=y_tar))

# ε is a threshold to indicate whether the generated samples are in high-density regions
tar_thrld = tf.divide(tf.reduce_max(D_prob_tar_gen[:,-1]) + tf.reduce_min(D_prob_tar_gen[:,-1]), 2)

indicator = tf.sign(tf.subtract(D_prob_tar_gen[:,-1], tar_thrld))  # judge P_data(G(z))> threshold
condition = tf.greater(tf.zeros_like(indicator), indicator)  # Complete definition of indicative function
mask_tar = tf.where(condition, tf.zeros_like(indicator), indicator) # if the indicator function condition is satisfied, there is a calculated value, otherwise it is 0 of the same shape
G_ent_loss = tf.reduce_mean(tf.multiply(tf.log(D_prob_tar_gen[:,-1]), mask_tar))

fm_loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(D_logit_real - D_logit_gen), 1)))

G_loss = pt_loss + G_ent_loss + fm_loss

D_solver = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(D_loss, var_list=theta_D)
G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)
T_solver = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(T_loss, var_list=theta_T)


# load data
min_max_scaler = MinMaxScaler() #for scaling values between 0-1 for decreasing std
x_benign = min_max_scaler.fit_transform(np.load('data/wiki/ben_hid_emd_4_50_8_200_r0.npy'))  # (10528, 200)
x_vandal = min_max_scaler.transform(np.load('data/wiki/val_hid_emd_4_50_8_200_r0.npy')) # (11495, 200)


x_benign = x_benign[:10000]
x_vandal = x_vandal[:10000]
x_pre = x_benign[:7000]

y_pre = np.zeros(len(x_pre)) # (7000, ) all 0
y_pre = one_hot(y_pre, 2) # (7000, 2) array([ [1, 0], [1, 0], ...,[1, 0]], dtype=int32)

x_train = x_pre # The training set is benign users

y_real_mb = one_hot(np.zeros(mb_size), 2)  # (mb_size, 2) array([[1, 0], [1, 0], ...,[1, 0]], dtype=int32)
y_fake_mb = one_hot(np.ones(mb_size), 2) # (100, 2) array([[0, 1], [0, 1], ...,[0, 1]], dtype=int32)

x_test = x_benign[-3000:].tolist() + x_vandal[-3000:].tolist()
x_test = np.array(x_test)

y_test = np.zeros(len(x_test))
y_test[3000:] = 1 # benign -- 0    vandal -- 1

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# pre-training for P_data() distribution
_ = sess.run(T_solver, feed_dict={X_tar:x_pre, y_tar:y_pre}) # Use benign data to train discriminator_T first

q = int(len(x_train) / mb_size)

d_ben_pro, d_fake_pro, fm_loss_coll = list(), list(), list()
f1_score = list()
d_val_pro = list()

epochs = 50

for n_epoch in range(epochs):
    X_mb_oc = sample_shuffle_uspv(x_train)
    for n_batch in range(q):
        # fix G，train D
        _, D_loss_curr, ent_real_curr = sess.run([D_solver, D_loss, ent_real_loss],
                                                 feed_dict={
                                                     X_oc: X_mb_oc[n_batch*mb_size:(n_batch+1)*mb_size], # benign data
                                                     Z: sample_Z(mb_size, Z_dim), # noise
                                                     y_real: y_real_mb, # labels of benign data
                                                     y_gen: y_fake_mb # labels of fake data
                                                 })
        # fix D，train G
        _, G_loss_curr, fm_loss_curr = sess.run([G_solver, G_loss, fm_loss], 
                                                feed_dict={
                                                    Z: sample_Z(mb_size, Z_dim),
                                                    X_oc: X_mb_oc[n_batch*mb_size:(n_batch+1)*mb_size], # Calculate the term fm_loss in G_loss
                                                })
    # After a round of training, calculate the predicted probability of D on the training set, of the generated data, and of the actual vandal data.
    # Used to draw curves and check the classification effect of D
    D_prob_real_, D_prob_gen_ = sess.run([D_prob_real, D_prob_gen],
                                         feed_dict={
                                             X_oc: x_train,
                                             Z: sample_Z(len(x_train), Z_dim)
                                         })
    D_prob_vandal_ = sess.run(D_prob_real, feed_dict={X_oc: x_vandal[0:7000]})
    
    d_ben_pro.append(np.mean(D_prob_real_[:, 0]))
    d_fake_pro.append(np.mean(D_prob_gen_[:, 0]))
    d_val_pro.append(np.mean(D_prob_vandal_[:, 0]))
    fm_loss_coll.append(fm_loss_curr)
    
    # Classification effect of test model D
    prob, _ = sess.run([D_prob_real, D_logit_real], feed_dict={X_oc: x_test})
    y_pred = np.argmax(prob, axis=1)
    conf_mat = classification_report(y_test, y_pred, target_names=['benign', 'vandal'], digits=4)
    f1_score.append(float(list(filter(None, conf_mat.strip().split(" ")))[12]))
    
acc = np.sum(y_pred == y_test)/float(len(y_pred))
print(conf_mat)
print("acc:%s"%acc)

draw_trend(d_ben_pro, d_fake_pro, d_val_pro, fm_loss_coll, f1_score)